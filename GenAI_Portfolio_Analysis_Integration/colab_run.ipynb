{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Multi-Modal Agentic RAG for Portfolio Stock Market Analysis\n",
    "\n",
    "## üî• Overview\n",
    "The **Multi-Modal Agentic RAG** is an AI-powered chatbot that enables users to interact with their **investment portfolios** using **natural language**. It leverages **Retrieval-Augmented Generation (RAG)** with **dynamic tool selection** to provide **real-time stock insights, portfolio management assistance, and financial news analysis**.\n",
    "\n",
    "## üöÄ Key Features\n",
    "- **üîç AI-Powered Chat with Portfolio Awareness**  \n",
    "  - Users can ask portfolio-specific questions like _\"How is my tech portfolio performing?\"_\n",
    "- **üì∞ Real-Time Market News Analysis**  \n",
    "  - Retrieves and summarizes **market trends, earnings reports, and investor call transcripts**\n",
    "- **üìä Technical & Fundamental Analysis**  \n",
    "  - Analyzes stock charts, moving averages, RSI, MACD, and valuation metrics\n",
    "- **‚öñÔ∏è Risk & Diversification Insights**  \n",
    "  - Evaluates sector exposure, volatility, and suggests diversification strategies\n",
    "- **üõ†Ô∏è Agentic Dynamic Tool Selection**  \n",
    "  - Selects the best tool (APIs, LLM, vector search) based on user query\n",
    "\n",
    "## üèóÔ∏è Architecture\n",
    "1Ô∏è‚É£ **LLM-Powered Conversational Agent** (GPT-4, FinBERT, BloombergGPT)  \n",
    "2Ô∏è‚É£ **RAG-Based Document Retrieval** (ChromaDB, FAISS, Weaviate)  \n",
    "3Ô∏è‚É£ **Market Data & Portfolio APIs** (Yahoo Finance, Alpha Vantage, Zerodha, Alpaca)  \n",
    "4Ô∏è‚É£ **Real-Time Stock Chart Analysis** (Matplotlib, Plotly, Chart.js)  \n",
    "5Ô∏è‚É£ **Dynamic Agent Orchestration** (LangChain, LlamaIndex)  \n",
    "\n",
    "## üì° Tech Stack\n",
    "- **Backend:** FastAPI / Flask / Node.js\n",
    "- **Frontend:** Next.js / React (for interactive dashboards)\n",
    "- **Database:** MongoDB / PostgreSQL (for storing user portfolios)\n",
    "- **APIs:** Yahoo Finance, Alpha Vantage, Alpaca, Zerodha\n",
    "- **LLM & NLP:** LLama3.1, Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cloning the Repository**\n",
    "\n",
    "To run the **GenAI Portfolio Analysis Integration** project, you need to first **clone the repository** from GitHub. This ensures that you have all the necessary files and dependencies required for execution.\n",
    "\n",
    "#### **Steps to Clone the Project**\n",
    "1. Open a terminal or command prompt.\n",
    "2. Run the following command to clone the repository:\n",
    "   ```sh\n",
    "   git clone https://github.com/mayank-cse1/AI-AlgoTrading.git\n",
    "   ```\n",
    "3. Navigate into the project directory:\n",
    "   ```sh\n",
    "   cd GenAI_Portfolio_Analysis_Integration\n",
    "   ```\n",
    "\n",
    "This step is **essential** because it pulls the latest codebase from GitHub, ensuring you have all required scripts, configurations, and dependencies needed to run the project smoothly. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mount Google Drive** in Google Colab, allowing access to files stored in Drive for loading datasets, saving models, and persisting project data. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changes the working directory** to a specified folder in Google Drive, ensuring that files are accessed and saved in the correct location. üìÇ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/ML_Notebooks/stock_market_analysis_2') # Change to your specific directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, when running asynchronous code inside a Jupyter notebook, you might encounter errors related to event loops. To prevent this and ensure smooth execution of async tasks, we apply a small fix that allows everything to run without conflicts. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that all necessary dependencies are installed, we use a command that reads from a `requirements.txt` file and installs all the listed packages. This helps set up the project environment quickly and ensures compatibility. ‚úÖüì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using **Sambanova LLM**, you need to set up an API key for authentication. This code securely stores the key as an environment variable, allowing the model to access it when making requests. üîë‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Use only of using Sambanova LLM\n",
    "import os\n",
    "os.environ[\"SAMBANOVA_API_KEY\"] = \"<SAMBANOVA_API_KEY>\"\n",
    "print(os.getenv(\"SAMBANOVA_API_KEY\"))  # Should print your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command **fetches your public IP address** using `wget` and displays it. It can be useful for network configuration, API security, or accessing remote resources. üåç‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command **runs a Streamlit app** (`app.py`) and then uses **LocalTunnel** to create a public URL, allowing external access to the app. Useful for sharing live demos! üöÄ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code **visualizes the possible execution flows** in a workflow-based system. It helps in understanding how different tools and the language model interact when processing user queries. Useful for debugging and optimizing the chatbot‚Äôs decision-making process! üîÑüìä‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from workflow import RouterOutputAgentWorkflow\n",
    "# Assuming RouterOutputAgentWorkflow is your workflow class\n",
    "workflow_instance = RouterOutputAgentWorkflow(\n",
    "    tools=[],  # Add your tools here\n",
    "    llm=None  # Add your LLM here\n",
    ")\n",
    "\n",
    "# Draw all possible flows for the given workflow\n",
    "draw_all_possible_flows(workflow_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service Level Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SambaNova API Testing - Chat Completion with Streaming**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Objective**\n",
    "This script tests the **SambaNova AI API** by sending a chat completion request using the **Meta-Llama-3.1-8B-Instruct** model. The response is streamed in real-time, allowing incremental output as the model generates text.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation Details**\n",
    "1Ô∏è‚É£ **API Request Setup**  \n",
    "   - The **API URL** is set to `https://api.sambanova.ai/v1/chat/completions`.  \n",
    "   - The **API key** is included in the `Authorization` header.  \n",
    "   - The request uses the `requests` library to send a **POST** request.\n",
    "\n",
    "2Ô∏è‚É£ **Payload (Request Body)**  \n",
    "   - The **model** used is `\"Meta-Llama-3.1-8B-Instruct\"`.  \n",
    "   - The **messages** array includes a system instruction and a user query.  \n",
    "   - `\"stream\": True` enables **real-time response streaming**.\n",
    "\n",
    "3Ô∏è‚É£ **Handling Streaming Response**  \n",
    "   - The response is read **line by line** to process streamed data.  \n",
    "   - Each chunk is parsed from JSON and prints the generated text.\n",
    "\n",
    "---\n",
    "\n",
    "### **Expected Output**\n",
    "Upon execution, the script should output a **happy story** in real-time, streaming it word-by-word.\n",
    "\n",
    "---\n",
    "\n",
    "### **Use Cases**\n",
    "- **Real-time chatbot responses**\n",
    "- **Low-latency conversational AI**\n",
    "- **Testing SambaNova API with different prompts**\n",
    "\n",
    "üöÄ **Modify the prompt and model to experiment with different AI behaviors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"https://api.sambanova.ai/v1/chat/completions\"\n",
    "API_KEY = \"your-secure-api-key\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Answer the question in a couple sentences.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Share a happy story with me\"}\n",
    "    ],\n",
    "    \"stream\": True  # Enable streaming\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=data, stream=True)\n",
    "\n",
    "for chunk in response.iter_lines():\n",
    "    if chunk:\n",
    "        print(json.loads(chunk.decode())[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ **Testing the Tool Calling Vector Index Function** \n",
    "\n",
    "This code **tests a vector-based retrieval system** that allows a chatbot to **search and retrieve relevant information** from user-provided documents.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è **Key Steps in the Code**  \n",
    "\n",
    "#### 1Ô∏è‚É£ **Importing Necessary Modules**  \n",
    "- Imports custom classes like `CustomLLM`, `Tool`, and `RouterOutputAgentWorkflow` from the project files.  \n",
    "- Imports `asyncio` for handling asynchronous execution.  \n",
    "\n",
    "#### 2Ô∏è‚É£ **Defining a Sample Document**  \n",
    "- Creates a `Document` object that contains a **personal statement** about an AI career journey.  \n",
    "- This document is **ingested into a vector database** for future semantic searches.  \n",
    "\n",
    "#### 3Ô∏è‚É£ **Setting Up Qdrant Vector Database**  \n",
    "- **QdrantVDB_QB** (a custom wrapper for Qdrant) is initialized.  \n",
    "- A **new collection** is created in Qdrant to store the document.  \n",
    "- The document is **embedded and stored** for later retrieval.  \n",
    "\n",
    "#### 4Ô∏è‚É£ **Defining the Vector Index Tool**  \n",
    "- A **semantic search tool (`vectorIndexTool`)** is created.  \n",
    "- This tool allows the system to **search for information** within the vector database based on user queries.  \n",
    "- It is described as a tool for fetching **investment strategies and market news**.  \n",
    "\n",
    "#### 5Ô∏è‚É£ **Initializing the Router Workflow**  \n",
    "- The `RouterOutputAgentWorkflow` is initialized with:  \n",
    "  - The `vectorIndexTool` for **retrieving personal investment insights**.  \n",
    "  - `CustomLLM()`, a custom language model for **interpreting and responding to user queries**.  \n",
    "  - A **timeout of 60 seconds** and **verbose mode enabled** for debugging.  \n",
    "\n",
    "#### 6Ô∏è‚É£ **Running a Test Query**  \n",
    "- The function `test_workflow()` sends a user query: **\"What is my name?\"**  \n",
    "- The workflow **retrieves relevant information** from the stored document in Qdrant.  \n",
    "- The **final response** is printed.  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Purpose of This Code**  \n",
    "‚úÖ Enables **semantic search** over personal documents & market news.  \n",
    "‚úÖ Connects a **vector database** with an agent-based retrieval system.  \n",
    "‚úÖ Integrates **LLM-powered decision-making** with vector search results.  \n",
    "‚úÖ Tests the **workflow execution** for responding to investment-related queries.  \n",
    "\n",
    "This setup allows users to **ask questions about their portfolio** and **get insights from stored market data**, making it a core component of the **multi-modal agentic RAG system**! üî•üí°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_code import OllamaLLM, Tool\n",
    "from workflow import RouterOutputAgentWorkflow\n",
    "import asyncio\n",
    "import asyncio\n",
    "from rag_code import  Retriever\n",
    "from rag_code import EmbedData, QdrantVDB_QB, Retriever\n",
    "from llama_index.core.schema import Document  # Correct import path\n",
    "document_text = \"Mayank Gupta (MS AI applicant for Fall 2025)\\nBackground/Passion\\nArtificial Intelligence is not just about models and algorithms; it is about transforming raw po-\\ntential into impact, about bridging the gap between what is theoretically possible and what can be\\nimplemented in the real world. Throughout my journey, I have always sought to implement what\\nI learn, ensuring that my knowledge is not confined to textbooks but actively contributes to solv-\\ning real-world challenges. The MSE-AI Online program at the University of Pennsylvania offers\\nprecisely the environment where I can refine this approach, delve deeper into AI technologies like\\ntransformers, and apply them to industry-specific problems at scale.\\nArtificial Intelligence is revolutionizing industries‚Äîoptimizing operations, saving billions of dol-\\nlars, and transforming lives. Yet, its true potential lies beyond efficiency; it is about crafting intel-\\nligent systems that elevate human capabilities and solve societal challenges. As an aspiring AI\\ninnovator, I seek to upskill through the MSE in Artificial Intelligence at the University of Pennsyl-\\nvania to push the boundaries of what AI can achieve.\\nA Journey Defined by Learning and Implementation\\nIn my four years of undergraduate studies, spanning 45 months, I dedicated 22 months to intern-\\nships‚Äîtesting, refining, and applying my learning in real-world settings. From my early days as a\\nresearch intern for two months, diving into AI-driven solutions, to designing a web application for a\\nlunch booking system at LG Electronics, my focus was always on creating something tangible from\\nwhat I learned.\\nAs I progressed, I ventured into NLP chatbot flows at Dewiride Technologies, working for five\\nmonths to refine conversational AI. My passion for problem-solving extended beyond industry ap-\\nplications‚ÄîI also took on the role of a Teaching Assistant for four months, helping students master\\ndata structures and algorithms, ensuring they not only understood concepts but could also imple-\\nment them effectively.\\nOne of the most significant chapters of my journey was my nine-month internship at Swiggy,\\nwhere I optimized payment technologies, contributing to Swiggy UPI Lite. These experiences rein-\\nforced my belief that true learning is not just about acquiring knowledge‚Äîit is about applying it to\\nsolve problems that matter.\\nPassion for AI: From Challenge to Innovation\\nMy fascination with AI was ignited during the Tamil Nadu Police Hackathon, where our challenge\\nwas to identify a thief from poor-quality surveillance footage using only a single reference image.\\nThis was not a controlled dataset; it was real-world chaos‚Äîgrainy, cluttered CCTV footage where\\neven the human eye struggled to discern details. Undeterred, we developed a Video-Face Tracking\\nSystem using Python and deep learning models such as MTCNN and ResNet-50, leveraging the\\nVGGFace2 dataset for face recognition. The true magic happened when our model detected intricate\\ndetails invisible to the naked eye, such as transparent spectacles on the suspect‚Äôs face. Although\\nwe did not win due to marginally lower accuracy than the top teams, our approach was widely\\nrecognized for its robustness in real-world conditions.\\nAnother milestone in my AI journey was the Bank of Baroda Hackathon, where my team and I\\ndeveloped ‚ÄôDev‚Äô, an AI-driven financial assistant. ‚ÄôDev‚Äô simplified financial management\"\n",
    "documents = [Document(text=document_text)]  # Explicitly set 'text' parameter\n",
    "\n",
    "qdrant_vdb = QdrantVDB_QB(collection_name=\"testing_mayank\")\n",
    "qdrant_vdb.define_client()\n",
    "qdrant_vdb.ingest_data(documents=documents)\n",
    "\n",
    "vectorIndexTool = Tool(\n",
    "    name=\"vectorIndexTool\",\n",
    "    description=\"A tool to do semantic search on user provided personal information. The document uploaded contains more user investment strategies and market news. Call it to get more context of user question. argument required = query \",\n",
    "    tool_id=\"vector_index_001\",\n",
    "    execute_fn= qdrant_vdb.query_data #lambda query: asyncio.run(qdrant_vdb.query_data.acall(query=query))  # Ensures proper execution\n",
    ")\n",
    "# Initialize RouterOutputAgentWorkflow with both tools\n",
    "workflow = RouterOutputAgentWorkflow(\n",
    "    tools=[vectorIndexTool],  # Include both tools\n",
    "    llm=CustomLLM(),  # Pass an LLM instance\n",
    "    timeout=60,\n",
    "    verbose=True\n",
    ")\n",
    "async def test_workflow():\n",
    "    user_query = \"What is my name?\"\n",
    "    result = await workflow.run(message=user_query)\n",
    "    print(\"Final Test Result:\", result)\n",
    "\n",
    "# Execute async function\n",
    "asyncio.run(test_workflow())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ **Explanation of the Code: Testing the SQL Function**  \n",
    "\n",
    "This code **tests a SQL-based retrieval function** that allows a chatbot to **query investment data stored in a structured database**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üèóÔ∏è **Key Steps in the Code**  \n",
    "\n",
    "#### 1Ô∏è‚É£ **Importing Required Libraries**  \n",
    "- `pandas` for handling structured tabular data.  \n",
    "- `asyncio` for handling **asynchronous execution**.  \n",
    "- **Custom classes** from `rag_code`:  \n",
    "  - `InvestmentPortfolioDB`: Manages an **investment portfolio database**.  \n",
    "  - `CustomLLM`: An LLM for **query interpretation**.  \n",
    "  - `Tool`: Defines a callable **text-to-SQL tool**.  \n",
    "  - `RouterOutputAgentWorkflow`: Manages **AI workflow execution**.  \n",
    "\n",
    "#### 2Ô∏è‚É£ **Creating Sample Investment Data**  \n",
    "- Defines a **dataset of stock investments** with:  \n",
    "  - `stock_symbol`: Ticker symbol (AAPL, GOOGL, etc.)  \n",
    "  - `quantity`: Number of shares owned.  \n",
    "  - `purchase_price`: The buying price per share.  \n",
    "  - `purchase_date`: The date of purchase.  \n",
    "- Converts the dataset into a **Pandas DataFrame**.  \n",
    "\n",
    "#### 3Ô∏è‚É£ **Initializing the SQL Database**  \n",
    "- **InvestmentPortfolioDB()** initializes an investment portfolio database.  \n",
    "- **create_table(df)** creates a structured table for storing stock data.  \n",
    "- **insert_data(df)** inserts the sample investment records into the database.  \n",
    "\n",
    "#### 4Ô∏è‚É£ **Defining the Text-to-SQL Tool**  \n",
    "- The `text_to_sql_tool` allows users to **query investment data** via natural language.  \n",
    "- This tool is responsible for **mapping user queries** (e.g., \"What is my investment in Apple?\") to **SQL queries** that retrieve relevant stock data.  \n",
    "\n",
    "#### 5Ô∏è‚É£ **Configuring the Workflow**  \n",
    "- **RouterOutputAgentWorkflow** is set up with:  \n",
    "  - `text_to_sql_tool`: For retrieving investment information.  \n",
    "  - `CustomLLM()`: To interpret and process the query.  \n",
    "  - `timeout=60`: Ensures workflow execution does not exceed **60 seconds**.  \n",
    "  - `verbose=True`: Enables **detailed logging** for debugging.  \n",
    "\n",
    "#### 6Ô∏è‚É£ **Running a Test Query**  \n",
    "- The function `test_workflow()` executes the **query \"What is my investment in Apple?\"**.  \n",
    "- The workflow **processes the question, translates it into an SQL query, fetches the relevant data**, and **returns the result**.  \n",
    "- The **final response** is printed.  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Purpose of This Code**  \n",
    "‚úÖ **Stores and retrieves structured investment data** using SQL.  \n",
    "‚úÖ Enables **natural language to SQL conversion** for financial queries.  \n",
    "‚úÖ Integrates **LLM-based decision-making** with structured database retrieval.  \n",
    "‚úÖ Tests **real-time execution** of chatbot queries on investment portfolios.  \n",
    "\n",
    "This setup allows users to **ask about their stock investments in plain English** and receive **structured financial insights** from the database. üî•üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "from rag_code import InvestmentPortfolioDB  # Ensure correct import\n",
    "from rag_code import CustomLLM, Tool\n",
    "from workflow import RouterOutputAgentWorkflow\n",
    "import asyncio\n",
    "import asyncio\n",
    "from rag_code import CustomLLM, Retriever\n",
    "from rag_code import EmbedData, QdrantVDB_QB, Retriever\n",
    "# Sample investment data\n",
    "data = {\n",
    "    \"stock_symbol\": [\"AAPL\", \"GOOGL\", \"MSFT\", \"TSLA\", \"AMZN\"],\n",
    "    \"quantity\": [10, 5, 12, 8, 15],\n",
    "    \"purchase_price\": [150.25, 2800.75, 305.5, 700.8, 3400.4],\n",
    "    \"purchase_date\": [\"2024-01-10\", \"2023-12-15\", \"2024-02-20\", \"2024-03-05\", \"2023-11-30\"]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize DB and insert data\n",
    "db = InvestmentPortfolioDB()\n",
    "db.create_table(df)\n",
    "db.insert_data(df)\n",
    "text_to_sql_tool = Tool(\n",
    "    name=\"text_to_sql_tool\",\n",
    "    description=\"A tool to query the structured document provided by the user. Call it to get understanding of user stock, mutual funds investments or realtime data. Argument required = user_question \",\n",
    "    tool_id=\"text_to_sql_001\",\n",
    "    execute_fn= db.sql_tool\n",
    ")\n",
    "# Initialize RouterOutputAgentWorkflow with both tools\n",
    "workflow = RouterOutputAgentWorkflow(\n",
    "    tools=[text_to_sql_tool],  # Include both tools\n",
    "    llm=CustomLLM(),  # Pass an LLM instance\n",
    "    timeout=60,\n",
    "    verbose=True\n",
    ")\n",
    "async def test_workflow():\n",
    "    user_query = \"What is my investment in apple\"\n",
    "    result = await workflow.run(message=user_query)\n",
    "    print(\"Final Test Result:\", result)\n",
    "\n",
    "# Execute async function\n",
    "asyncio.run(test_workflow())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact\n",
    "For any queries, reach out at: [mayank.guptacse1@gmail.com](mailto:mayank.guptacse1@gmail.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
